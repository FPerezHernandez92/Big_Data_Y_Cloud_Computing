Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.2
      /_/

Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51)
Type in expressions to have them evaluated.
Type :help for more information.
Spark context available as sc.
SQL context available as sqlContext.

scala> //Mostrar solamente los mensajes de ERROR

scala> import org.apache.log4j.{Level, Logger}
import org.apache.log4j.{Level, Logger}

scala> var rootLogger = Logger.getRootLogger()
rootLogger: org.apache.log4j.Logger = org.apache.log4j.spi.RootLogger@40e4dd1a

scala> rootLogger.setLevel(Level.WARN)

scala> //Importación de las librerías

scala> import org.apache.spark.SparkContext
import org.apache.spark.SparkContext

scala> import org.apache.spark.SparkConf
import org.apache.spark.SparkConf

scala> import org.apache.spark.rdd._
import org.apache.spark.rdd._

scala> import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Vectors

scala> import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.regression.LabeledPoint

scala> import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.mllib.evaluation.MulticlassMetrics

scala> import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

scala> import org.apache.spark.mllib.classification.kNN_IS.kNN_IS
import org.apache.spark.mllib.classification.kNN_IS.kNN_IS

scala> import utils.keel.KeelParser
import utils.keel.KeelParser

scala> import scala.collection.mutable.ListBuffer
import scala.collection.mutable.ListBuffer

scala> //Random Forest

scala> import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.RandomForest

scala> import org.apache.spark.mllib.tree.model.RandomForestModel
import org.apache.spark.mllib.tree.model.RandomForestModel

scala> import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.util.MLUtils

scala> //Naive Bayes

scala> import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel }
import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}

scala> import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Vectors

scala> //Decision Trees

scala> import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.DecisionTree

scala> import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.tree.model.DecisionTreeModel

scala> import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.util.MLUtils

scala> //Discretizar Valores

scala> import org.apache.spark.mllib.feature.MDLPDiscretizer
import org.apache.spark.mllib.feature.MDLPDiscretizer

scala> //Selección de características

scala> import org.apache.spark.mllib.feature._
import org.apache.spark.mllib.feature._

scala> //Fichero para imprimir las salidas

scala> import java.io._
import java.io._

scala> val writer = new PrintWriter(new File("2seleccion10salida.txt"))
writer: java.io.PrintWriter = java.io.PrintWriter@142a3c26

scala> //Obtenemos los resultados

scala> import org.apache.spark.mllib.evaluation._
import org.apache.spark.mllib.evaluation._

scala> 

scala> 

scala> //Carga de los datasets

scala> val converter = new KeelParser(sc, "hdfs://hadoop-master/user/spark/datas ets/ECBDL14_mbd/ecbdl14.header")
converter: utils.keel.KeelParser = utils.keel.KeelParser@50b6999a

scala> val train = sc.textFile("hdfs://hadoop-master/user/spark/datasets/ECBDL14 _mbd/ecbdl14tra.data", 6).map(line => converter.parserToLabeledPoint(line)).coal esce(6).cache()
train: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = CoalescedRDD[5] at coalesce at <console>:61

scala> writer.write("En train las instancias son: " + train.count() + "\n")

scala> val test = sc.textFile("hdfs://hadoop-master/user/spark/datasets/ECBDL14_ mbd/ecbdl14tst.data", 6).map(line => converter.parserToLabeledPoint(line)).cache ()
test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at map at <console>:61

scala> writer.write("En test las instancias son: " + test.count() + "\n")

scala> 

scala> 

scala> //DISCRETIZACIÓN DE LOS DATOS

scala> writer.write("Comienzo Discretización de los Datos con los siguientes par ámetros:" + "\n")

scala> val tiempo_inicial = System.currentTimeMillis / 1000
tiempo_inicial: Long = 1494014156

scala> val categoricalFeat: Option[Seq[Int]] = None
categoricalFeat: Option[Seq[Int]] = None

scala> val nBins = 15
nBins: Int = 15

scala> val maxByPart = 10000
maxByPart: Int = 10000

scala> writer.write("\nDiscretización de los datos\n")

scala> writer.write("\t*** Discretization method: Fayyad discretizer (MDLP)\n")

scala> writer.write("\tcategoricalFeat: Option[Seq[Int]] = None \n")

scala> writer.write("\tNumber of bins: " + nBins + "\n")

scala> writer.write("\tMax By Part: " + maxByPart + "\n")

scala> //Los datos deben almacenarse en cache para mejorar el rendimiento

scala> val discretizer = MDLPDiscretizer.train(train, // RDD[LabeledPoint]
     |     categoricalFeat, // continuous features
     |     nBins, // max number of thresholds by feature
     |     maxByPart) // max elements per partition
discretizer: org.apache.spark.mllib.feature.DiscretizerModel = org.apache.spark.mllib.feature.DiscretizerModel@76c4f53f

scala> discretizer
res9: org.apache.spark.mllib.feature.DiscretizerModel = org.apache.spark.mllib.feature.DiscretizerModel@76c4f53f

scala> //Discretizamos el train

scala> val discreteTrain = train.map(i => LabeledPoint(i.label, discretizer.tran sform(i.features))).cache()
discreteTrain: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[38] at map at <console>:71

scala> discreteTrain.first()
res10: org.apache.spark.mllib.regression.LabeledPoint = (0.0,[5.0,5.0,3.0,2.0,2.0,2.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,0.0,0.0,0.0,0.0,13.0,2.0,2.0,2.0,4.0,11.0,15.0,4.0,1.0,0.0,12.0,14.0,13.0,12.0,4.0,6.0,5.0,0.0,0.0,0.0,2.0,2.0,2.0,2.0,3.0,2.0,1.0,1.0,1.0,0.0,0.0,2.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0,2.0,3.0,2.0,3.0,3.0,3.0,2.0,1.0,1.0,0.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,2.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,12.0,13.0,3.0,11.0,1.0,10.0,5.0,13.0,12.0,1.0,11.0,5.0,11.0,8.0,6.0,9.0,9.0,12.0,5.0,2.0,3.0,3.0,0.0,12.0,2.0,3.0,9.0,10.0,10.0,3.0,0.0,2.0,6.0,4.0,3.0,0.0,2.0,6.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,13.0,14.0,5.0,15.0,0.0,6.0,4.0,4.0,11.0,11.0,15.0,2.0,5.0,4.0,11.0,7.0,2.0,5.0,7.0,11.0,2.0,2.0,1.0,3.0,0.0,2.0,...
scala> //Discretizamos el test

scala> val discreteTest = test.map(i => LabeledPoint(i.label, discretizer.transf orm(i.features))).cache()
discreteTest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[39] at map at <console>:73

scala> val segundos = (System.currentTimeMillis / 1000) - tiempo_inicial
segundos: Long = 43

scala> val minutos = segundos / 60
minutos: Long = 0

scala> writer.write("\nFin de Discretización de los datos con una duración de: "  + segundos + " segundos o de " +  minutos + " minutos" + "\n")

scala> 

scala> 

scala> //SELECCIÓN DE CARACTERÍSTICAS SOBRE LOS DATOS DISCRETIZADOS

scala> writer.write("Comienzo Selección de características con los datos discret izados y los siguientes parámetros:" + "\n")

scala> val tiempo_inicial = System.currentTimeMillis / 1000
tiempo_inicial: Long = 1494014200

scala> val criterion = new InfoThCriterionFactory("mrmr")
criterion: org.apache.spark.mllib.feature.InfoThCriterionFactory = org.apache.spark.mllib.feature.InfoThCriterionFactory@4eaa358d

scala> val nToSelect = 10
nToSelect: Int = 10

scala> val nPartitions = 6
nPartitions: Int = 6

scala> writer.write("\t*** FS criterion: " + criterion.getCriterion.toString)

scala> writer.write("\n\tNumber of features to select: " + nToSelect)

scala> writer.write("\n\tNumber of partitions: " + nPartitions)

scala> //Discretizamos con 6 particiones y 10 características

scala> val featureSelector = new InfoThSelector(criterion, nToSelect, nPartition s).fit(discreteTrain)

*** MaxRel features ***
Feature	Score
1	0.0762
3	0.0382
124	0.0377
120	0.0373
116	0.0356
22	0.0343
129	0.0341
130	0.0335
117	0.0331
125	0.0324
featureSelector: org.apache.spark.mllib.feature.InfoThSelectorModel = org.apache.spark.mllib.feature.InfoThSelectorModel@10f8aea3

scala> val reducedTrain10 = discreteTrain.map(i => LabeledPoint(i.label, feature Selector.transform(i.features))).cache()
reducedTrain10: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[120] at map at <console>:81

scala> //Ahora el test

scala> val reducedTest10 = discreteTrain.map(i => LabeledPoint(i.label, featureS elector.transform(i.features))).cache()
reducedTest10: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[121] at map at <console>:81

scala> val segundos = (System.currentTimeMillis / 1000) - tiempo_inicial
segundos: Long = 39

scala> val minutos = segundos / 60
minutos: Long = 0

scala> writer.write("\nFin de selección de características con los datos discret izados y con una duración de: " + segundos + " segundos o de " +  minutos + " mi nutos" + "\n")

scala> 

scala> 

scala> //RANDOM FOREST 2 CON SELECCIÓN DE CARACTERÍSTICAS

scala> writer.write("\nComienzo Random Forest 2 (Selección de características) c on los siguientes parámetros:" + "\n")

scala> val tiempo_inicial = System.currentTimeMillis / 1000
tiempo_inicial: Long = 1494014239

scala> val numClasses = 2
numClasses: Int = 2

scala> val categoricalFeaturesInfo = Map[Int, Int]()
categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()

scala> val numTrees = 10 
numTrees: Int = 10

scala> val featureSubsetStrategy = "auto" 
featureSubsetStrategy: String = auto

scala> val impurity = "gini"
impurity: String = gini

scala> val maxDepth = 4
maxDepth: Int = 4

scala> val maxBins = 32
maxBins: Int = 32

scala> writer.write("\tNumClasses: " + numClasses + "\n\tcategoricalFeaturesInfo : Map[Int, Int]()" + "\n\tnumTrees: " + numTrees + "\n\tfeatureSubsetStrategy: "  + featureSubsetStrategy + "\n\timpurity: " + impurity + "\n\tmaxDepth: " + maxD epth + "\n\tmaxBins: " + maxBins + "\n")

scala> val model = RandomForest.trainClassifier(reducedTrain10, numClasses, cate goricalFeaturesInfo,
     |   numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)
model: org.apache.spark.mllib.tree.model.RandomForestModel = 
TreeEnsembleModel classifier with 10 trees


scala> //Evaluamos el modelo con el test

scala> val predsAndLabels = reducedTest10.map { point =>
     |   val prediction = model.predict(point.features)
     |   (prediction, point.label)
     | }
predsAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[142] at map at <console>:101

scala> //Obtenemos el test error

scala> writer.write("\nResultados:")

scala> val testErr_rf2 = predsAndLabels.filter(r => r._1 != r._2).count.toDouble  / reducedTest10.count()
testErr_rf2: Double = 0.2739418839561196

scala> writer.write("\n\tTest Error= " + testErr_rf2)

scala> //println("Learned model:\n" + model.toDebugString) //Mostramos el modelo  COMENTADO

scala> //Mostramos los resultados pormenorizados

scala> val metrics_rf2 = new MulticlassMetrics(predsAndLabels)
metrics_rf2: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@19d3f271

scala> writer.write("\n\tPrecisión: " + metrics_rf2.precision)

scala> writer.write("\n\tMatriz de confusión: \n" + metrics_rf2.confusionMatrix  )

scala> val binaryMetrics_rf2 = new BinaryClassificationMetrics(predsAndLabels)
binaryMetrics_rf2: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@27b7054f

scala> val area_under_ROC_rf2 = binaryMetrics_rf2.areaUnderROC
area_under_ROC_rf2: Double = 0.5820332154648138

scala> writer.write("\n\tArea bajo la curva ROC = " + area_under_ROC_rf2)

scala> val segundos = (System.currentTimeMillis / 1000) - tiempo_inicial
segundos: Long = 10

scala> val minutos = segundos / 60
minutos: Long = 0

scala> writer.write("\nFin de Random Forest 2 (selección de características) con  una duración de: " + segundos + " segundos o de " +  minutos + " minutos" + "\n \n")

scala> 

scala> 

scala> //NAIVE BAYES 2 CON SELECCIÓN DE CARACTERÍSTICAS

scala> writer.write("Comienzo Naive Bayes 2 (Selección de Características) con l os siguientes parámetros:" + "\n")

scala> val tiempo_inicial = System.currentTimeMillis / 1000
tiempo_inicial: Long = 1494014249

scala> //Creamos el modelo

scala> val miLambda = 1.0
miLambda: Double = 1.0

scala> val miModelType = "multinomial"
miModelType: String = multinomial

scala> writer.write("\tlambda: " + miLambda + "\n\tmodelType: " + miModelType +  "\n")

scala> val model = NaiveBayes.train(reducedTrain10, lambda = miLambda, modelType  = miModelType)
model: org.apache.spark.mllib.classification.NaiveBayesModel = org.apache.spark.mllib.classification.NaiveBayesModel@6cbb0855

scala> //Evaluamos el modelo

scala> val predsAndLabels = reducedTest10.map { point =>
     |   val prediction = model.predict(point.features)
     |   (prediction, point.label)
     | }
predsAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[168] at map at <console>:91

scala> //Obtenemos el test error

scala> writer.write("\nResultados:")

scala> val testErr_nb2 = predsAndLabels.filter(r => r._1 != r._2).count.toDouble  / reducedTest10.count()
testErr_nb2: Double = 0.284933997264986

scala> writer.write("\n\tTestError= " + testErr_nb2)

scala> //Mostramos los resultados pormenorizados

scala> val metrics_nb2 = new MulticlassMetrics(predsAndLabels)
metrics_nb2: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@ea03030

scala> writer.write("\n\tPrecisión: " + metrics_nb2.precision)

scala> writer.write("\n\tMatriz de confusión: \n" + metrics_nb2.confusionMatrix  )

scala> val binaryMetrics_nb2 = new BinaryClassificationMetrics(predsAndLabels)
binaryMetrics_nb2: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@2a9b11fb

scala> val area_under_ROC_nb2 = binaryMetrics_nb2.areaUnderROC
area_under_ROC_nb2: Double = 0.6523599677232557

scala> writer.write("\n\tArea bajo la curva ROC = " + area_under_ROC_nb2)

scala> val segundos = (System.currentTimeMillis / 1000) - tiempo_inicial
segundos: Long = 3

scala> val minutos = segundos / 60
minutos: Long = 0

scala> writer.write("\nFin de Naive Bayes 2 con una duración de: " + segundos +  " segundos o de " +  minutos + " minutos" + "\n")

scala> 

scala> 

scala> //DECISION TREES 2 CON SELECCIÓN DE CARACTERÍSTICAS

scala> writer.write("Comienzo Decision Trees 1 con los siguientes parámetros:" +  "\n")

scala> val tiempo_inicial = System.currentTimeMillis / 1000
tiempo_inicial: Long = 1494014252

scala> //Configuramos el modelo y lo entrenamos

scala> val numClasses = 2
numClasses: Int = 2

scala> val categoricalFeaturesInfo = Map[Int, Int]()
categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()

scala> val impurity = "gini"
impurity: String = gini

scala> val maxDepth = 4
maxDepth: Int = 4

scala> val maxBins = 32
maxBins: Int = 32

scala> writer.write("\tNumClasses: " + numClasses + "\n\tcategoricalFeaturesInfo : Map[Int, Int]()" + "\n\timpurity: " + impurity + "\n\tmaxDepth: " + maxDepth +  "\n\tmaxBins: " + maxBins + "\n")

scala> val model = DecisionTree.trainClassifier(reducedTrain10, numClasses, cate goricalFeaturesInfo,
     |   impurity, maxDepth, maxBins)
model: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 4 with 31 nodes

scala> //Evaluamos el modelo con el test

scala> val predsAndLabels = reducedTest10.map { point =>
     |   val prediction = model.predict(point.features)
     |   (prediction, point.label)
     | }
predsAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[212] at map at <console>:97

scala> //Obtenemos el test error

scala> writer.write("\nResultados:")

scala> val testErr_dt2 = predsAndLabels.filter(r => r._1 != r._2).count.toDouble  / reducedTest10.count() 
testErr_dt2: Double = 0.2718976311895709

scala> writer.write("\n\tTestError= " + testErr_dt2)

scala> //println("Learned model:\n" + model.toDebugString) //Mostramos el modelo 

scala> //Mostramos los resultados pormenorizados

scala> val metrics_dt2 = new MulticlassMetrics(predsAndLabels)
metrics_dt2: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@651c1f85

scala> writer.write("\n\tPrecisión: " + metrics_dt2.precision)

scala> writer.write("\n\tMatriz de confusión: \n" + metrics_dt2.confusionMatrix  )

scala> val binaryMetrics_dt2 = new BinaryClassificationMetrics(predsAndLabels)
binaryMetrics_dt2: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@114e102e

scala> val area_under_ROC_dt2 = binaryMetrics_dt2.areaUnderROC
area_under_ROC_dt2: Double = 0.6190948021138991

scala> writer.write("\n\tArea bajo la curva ROC = " + area_under_ROC_dt2)

scala> val segundos = (System.currentTimeMillis / 1000) - tiempo_inicial
segundos: Long = 4

scala> val minutos = segundos / 60
minutos: Long = 0

scala> writer.write("\nFin de Decision Trees 2 con una duración de: " + segundos  + " segundos o de " +  minutos + " minutos" + "\n")

scala> 

scala> writer.close()

scala> 

scala> 

scala> exit
warning: there were 1 deprecation warning(s); re-run with -deprecation for details
