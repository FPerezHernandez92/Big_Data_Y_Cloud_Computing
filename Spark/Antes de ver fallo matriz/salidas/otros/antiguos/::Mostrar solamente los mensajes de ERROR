//Mostrar solamente los mensajes de ERROR
import org.apache.log4j.{Level, Logger}
var rootLogger = Logger.getRootLogger()
rootLogger.setLevel(Level.WARN)
//Importación de las librerías
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.rdd._
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.classification.kNN_IS.kNN_IS
import utils.keel.KeelParser
import scala.collection.mutable.ListBuffer
//Random Forest
import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.model.RandomForestModel
import org.apache.spark.mllib.util.MLUtils
//Naive Bayes
import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}
import org.apache.spark.mllib.linalg.Vectors
//Decision Trees
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.util.MLUtils
//Discretizar Valores
import org.apache.spark.mllib.feature.MDLPDiscretizer
//Selección de características
import org.apache.spark.mllib.feature._
//Fichero para imprimir las salidas
import java.io._
val writer = new PrintWriter(new File("pppr.txt"))
//Obtenemos los resultados
import org.apache.spark.mllib.evaluation._


//Carga de los datasets
val converter = new KeelParser(sc, "hdfs://hadoop-master/user/spark/datasets/ECBDL14_mbd/ecbdl14.header")
val train = sc.textFile("hdfs://hadoop-master/user/spark/datasets/ECBDL14_mbd/ecbdl14tra.data", 6).map(line => converter.parserToLabeledPoint(line)).coalesce(6).cache()
writer.write("En train las instancias son: " + train.count() + "\n")
val test = sc.textFile("hdfs://hadoop-master/user/spark/datasets/ECBDL14_mbd/ecbdl14tst.data", 6).map(line => converter.parserToLabeledPoint(line)).cache()
writer.write("En test las instancias son: " + test.count() + "\n")
//Para saber el número de características que tenemos:
writer.write("El número de características en train es: " + train.map(_.features.size).first() + "\n")
writer.write("El número de características en test es: " + test.map(_.features.size).first() + "\n")
//Para saber cuantos elementos hay en cada clase:
writer.write("El número de elementos de cada clase en train es: " + train.map(_.label).countByValue() + "\n")
writer.write("El número de elementos de cada clase en test es: " + test.map(_.label).countByValue() + "\n")


//DISCRETIZACIÓN DE LOS DATOS
writer.write("\nComienzo Discretización de los Datos con los siguientes parámetros:" + "\n")
val tiempo_inicial = System.currentTimeMillis / 1000
val categoricalFeat: Option[Seq[Int]] = None
val nBins = 15
val maxByPart = 10000
writer.write("\t*** Discretization method: Fayyad discretizer (MDLP)\n")
writer.write("\tcategoricalFeat: Option[Seq[Int]] = None \n")
writer.write("\tNumber of bins: " + nBins + "\n")
writer.write("\tMax By Part: " + maxByPart + "\n")
//Los datos deben almacenarse en cache para mejorar el rendimiento
val discretizer = MDLPDiscretizer.train(train, // RDD[LabeledPoint]
    categoricalFeat, // continuous features
    nBins, // max number of thresholds by feature
    maxByPart) // max elements per partition
discretizer
//Discretizamos el train
val discreteTrain = train.map(i => LabeledPoint(i.label, discretizer.transform(i.features))).cache()
discreteTrain.first()
//Discretizamos el test
val discreteTest = test.map(i => LabeledPoint(i.label, discretizer.transform(i.features))).cache()
val segundos = (System.currentTimeMillis / 1000) - tiempo_inicial
val minutos = segundos / 60
writer.write("Fin de Discretización de los datos con una duración de: " + segundos + " segundos o de " +  minutos + " minutos" + "\n\n")


//SELECCIÓN DE CARACTERÍSTICAS SOBRE LOS DATOS DISCRETIZADOS
writer.write("Comienzo Selección de características con los datos discretizados y los siguientes parámetros:" + "\n")
val tiempo_inicial = System.currentTimeMillis / 1000
val criterion = new InfoThCriterionFactory("mrmr")
val nToSelect = 10
val nPartitions = 6
writer.write("\t*** FS criterion: " + criterion.getCriterion.toString)
writer.write("\n\tNumber of features to select: " + nToSelect)
writer.write("\n\tNumber of partitions: " + nPartitions)
//Discretizamos con 6 particiones y 10 características
val featureSelector = new InfoThSelector(criterion, nToSelect, nPartitions).fit(discreteTrain)
val reducedTrain10 = discreteTrain.map(i => LabeledPoint(i.label, featureSelector.transform(i.features))).cache()
//Ahora el test
val reducedTest10 = discreteTest.map(i => LabeledPoint(i.label, featureSelector.transform(i.features))).cache()
val segundos = (System.currentTimeMillis / 1000) - tiempo_inicial
val minutos = segundos / 60
writer.write("\nEn el nuevo train las instancias son: " + reducedTrain10.count() + "\n")
writer.write("En el nuevo test las instancias son: " + reducedTest10.count())
writer.write("\nFin de selección de características con los datos discretizados y con una duración de: " + segundos + " segundos o de " +  minutos + " minutos" + "\n")


val converterrr = sc.textFile("hdfs://hadoop-master/user/spark/datasets/ECBDL14_mbd/ecbdl14.header")


val converterrr = sc.textFile("hdfs://hadoop-master/user/spark/datasets/ECBDL14_mbd/ecbdl14.header", 6).map(line => converter.parserToLabeledPoint(line)).coalesce(6).cache()

val converter10 = converterrr.map(i => LabeledPoint(i.label, featureSelector.transform(i.features))).cache()
converter10.repartition(6).coalesce(1, shuffle = true).saveAsTextFile("hdfs://hadoop-master/user/mdat20076629/DTSpark/dt.header")



reducedTrain10.repartition(6).coalesce(1, shuffle = true).saveAsTextFile("hdfs://hadoop-master/user/mdat20076629/DTSpark/train_overd10.data")


reducedTest10.repartition(6).coalesce(1, shuffle = true).saveAsTextFile("hdfs://hadoop-master/user/mdat20076629/DTSpark/test_overd10.data")




/opt/spark-1.6.2/bin/spark-submit --class org.apache.spark.mllib.sampling.runROS Imb-sampling-1.1.jar hdfs://hadoop-master/user/mdat20076629/DTSpark/test_overd10.data hdfs://hadoop-master/user/mdat20076629/DTSpark/test_overd10.data 100 250 0 1 2 hdfs://hadoop-master/user/mdat20076629/DTSpark/tst_ROS10.data





