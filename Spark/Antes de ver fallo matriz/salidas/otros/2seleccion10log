Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.2
      /_/

Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51)
Type in expressions to have them evaluated.
Type :help for more information.
Spark context available as sc.
SQL context available as sqlContext.

scala> //Mostrar solamente los mensajes de ERROR

scala> import org.apache.log4j.{Level, Logger}
import org.apache.log4j.{Level, Logger}

scala> var rootLogger = Logger.getRootLogger()
rootLogger: org.apache.log4j.Logger = org.apache.log4j.spi.RootLogger@750a3102

scala> rootLogger.setLevel(Level.WARN)

scala> //Importación de las librerías

scala> import org.apache.spark.SparkContext
import org.apache.spark.SparkContext

scala> import org.apache.spark.SparkConf
import org.apache.spark.SparkConf

scala> import org.apache.spark.rdd._
import org.apache.spark.rdd._

scala> import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Vectors

scala> import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.regression.LabeledPoint

scala> import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.mllib.evaluation.MulticlassMetrics

scala> import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

scala> import org.apache.spark.mllib.classification.kNN_IS.kNN_IS
import org.apache.spark.mllib.classification.kNN_IS.kNN_IS

scala> import utils.keel.KeelParser
import utils.keel.KeelParser

scala> import scala.collection.mutable.ListBuffer
import scala.collection.mutable.ListBuffer

scala> //Random Forest

scala> import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.RandomForest

scala> import org.apache.spark.mllib.tree.model.RandomForestModel
import org.apache.spark.mllib.tree.model.RandomForestModel

scala> import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.util.MLUtils

scala> //Naive Bayes

scala> import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}
import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}

scala> import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Vectors

scala> //Decision Trees

scala> import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.DecisionTree

scala> import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.tree.model.DecisionTreeModel

scala> import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.util.MLUtils

scala> //Discretizar Valores

scala> import org.apache.spark.mllib.feature.MDLPDiscretizer
import org.apache.spark.mllib.feature.MDLPDiscretizer

scala> //Selección de características

scala> import org.apache.spark.mllib.feature._
import org.apache.spark.mllib.feature._

scala> //Fichero para imprimir las salidas

scala> import java.io._
import java.io._

scala> val writer = new PrintWriter(new File("2seleccion10salida.txt"))
writer: java.io.PrintWriter = java.io.PrintWriter@53cb33fc

scala> //Obtenemos los resultados

scala> import org.apache.spark.mllib.evaluation._
import org.apache.spark.mllib.evaluation._

scala> 

scala> 

scala> //Carga de los datasets

scala> val converter = new KeelParser(sc, "hdfs://hadoop-master/user/spark/datasets/ECBDL1 4_mbd/ecbdl14.header")
converter: utils.keel.KeelParser = utils.keel.KeelParser@d9313d

scala> val train = sc.textFile("hdfs://hadoop-master/user/spark/datasets/ECBDL14_mbd/ecbdl 14tra.data", 6).map(line => converter.parserToLabeledPoint(line)).coalesce(6).cache()
train: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = CoalescedRDD[5] at coalesce at <console>:61

scala> writer.write("En train las instancias son: " + train.count() + "\n")

scala> val test = sc.textFile("hdfs://hadoop-master/user/spark/datasets/ECBDL14_mbd/ecbdl1 4tst.data", 6).map(line => converter.parserToLabeledPoint(line)).cache()
test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at map at <console>:61

scala> writer.write("En test las instancias son: " + test.count() + "\n")

scala> //Para saber el número de características que tenemos:

scala> writer.write("El número de características en train es: " + train.map(_.features.si ze).first() + "\n")

scala> writer.write("El número de características en test es: " + test.map(_.features.size ).first() + "\n")

scala> //Para saber cuantos elementos hay en cada clase:

scala> writer.write("El número de elementos de cada clase en train es: " + train.map(_.lab el).countByValue() + "\n")

scala> writer.write("El número de elementos de cada clase en test es: " + test.map(_.label ).countByValue() + "\n")

scala> 

scala> 

scala> //DISCRETIZACIÓN DE LOS DATOS

scala> writer.write("\nComienzo Discretización de los Datos con los siguientes parámetros: " + "\n")

scala> val tiempo_inicial = System.currentTimeMillis / 1000
tiempo_inicial: Long = 1494922461

scala> val categoricalFeat: Option[Seq[Int]] = None
categoricalFeat: Option[Seq[Int]] = None

scala> val nBins = 15
nBins: Int = 15

scala> val maxByPart = 10000
maxByPart: Int = 10000

scala> writer.write("\t*** Discretization method: Fayyad discretizer (MDLP)\n")

scala> writer.write("\tcategoricalFeat: Option[Seq[Int]] = None \n")

scala> writer.write("\tNumber of bins: " + nBins + "\n")

scala> writer.write("\tMax By Part: " + maxByPart + "\n")

scala> //Los datos deben almacenarse en cache para mejorar el rendimiento

scala> val discretizer = MDLPDiscretizer.train(train, // RDD[LabeledPoint]
     |     categoricalFeat, // continuous features
     |     nBins, // max number of thresholds by feature
     |     maxByPart) // max elements per partition
discretizer: org.apache.spark.mllib.feature.DiscretizerModel = org.apache.spark.mllib.feature.DiscretizerModel@5f35a7c

scala> discretizer
res12: org.apache.spark.mllib.feature.DiscretizerModel = org.apache.spark.mllib.feature.DiscretizerModel@5f35a7c

scala> //Discretizamos el train

scala> val discreteTrain = train.map(i => LabeledPoint(i.label, discretizer.transform(i.fe atures))).cache()
discreteTrain: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[48] at map at <console>:71

scala> discreteTrain.first()
res13: org.apache.spark.mllib.regression.LabeledPoint = (0.0,[5.0,5.0,3.0,2.0,2.0,2.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,0.0,0.0,0.0,0.0,13.0,2.0,2.0,2.0,4.0,11.0,15.0,4.0,1.0,0.0,12.0,14.0,13.0,12.0,4.0,6.0,5.0,0.0,0.0,0.0,2.0,2.0,2.0,2.0,3.0,2.0,1.0,1.0,1.0,0.0,0.0,2.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0,2.0,3.0,2.0,3.0,3.0,3.0,2.0,1.0,1.0,0.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,2.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,12.0,13.0,3.0,11.0,1.0,10.0,5.0,13.0,12.0,1.0,11.0,5.0,11.0,8.0,6.0,9.0,9.0,12.0,5.0,2.0,3.0,3.0,0.0,12.0,2.0,3.0,9.0,10.0,10.0,3.0,0.0,2.0,6.0,4.0,3.0,0.0,2.0,6.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,13.0,14.0,5.0,15.0,0.0,6.0,4.0,4.0,11.0,11.0,15.0,2.0,5.0,4.0,11.0,7.0,2.0,5.0,7.0,11.0,2.0,2.0,1.0,3.0,0.0,2.0,...
scala> //Discretizamos el test

scala> val discreteTest = test.map(i => LabeledPoint(i.label, discretizer.transform(i.feat ures))).cache()
discreteTest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[49] at map at <console>:73

scala> val segundos = (System.currentTimeMillis / 1000) - tiempo_inicial
segundos: Long = 40

scala> val minutos = segundos / 60
minutos: Long = 0

scala> writer.write("Fin de Discretización de los datos con una duración de: " + segundos  + " segundos o de " +  minutos + " minutos" + "\n\n")

scala> 

scala> 

scala> //SELECCIÓN DE CARACTERÍSTICAS SOBRE LOS DATOS DISCRETIZADOS

scala> writer.write("Comienzo Selección de características con los datos discretizados y l os siguientes parámetros:" + "\n")

scala> val tiempo_inicial = System.currentTimeMillis / 1000
tiempo_inicial: Long = 1494922501

scala> val criterion = new InfoThCriterionFactory("mrmr")
criterion: org.apache.spark.mllib.feature.InfoThCriterionFactory = org.apache.spark.mllib.feature.InfoThCriterionFactory@209b4bd

scala> val nToSelect = 10
nToSelect: Int = 10

scala> val nPartitions = 6
nPartitions: Int = 6

scala> writer.write("\t*** FS criterion: " + criterion.getCriterion.toString)

scala> writer.write("\n\tNumber of features to select: " + nToSelect)

scala> writer.write("\n\tNumber of partitions: " + nPartitions)

scala> //Discretizamos con 6 particiones y 10 características

scala> val featureSelector = new InfoThSelector(criterion, nToSelect, nPartitions).fit(dis creteTrain)

*** MaxRel features ***
Feature	Score
1	0.0762
3	0.0382
124	0.0377
120	0.0373
116	0.0356
22	0.0343
129	0.0341
130	0.0335
117	0.0331
125	0.0324
featureSelector: org.apache.spark.mllib.feature.InfoThSelectorModel = org.apache.spark.mllib.feature.InfoThSelectorModel@4b6e96d

scala> val reducedTrain10 = discreteTrain.map(i => LabeledPoint(i.label, featureSelector.t ransform(i.features))).cache()
reducedTrain10: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[130] at map at <console>:81

scala> //Ahora el test

scala> val reducedTest10 = discreteTest.map(i => LabeledPoint(i.label, featureSelector.tra nsform(i.features))).cache()
reducedTest10: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[131] at map at <console>:85

scala> val segundos = (System.currentTimeMillis / 1000) - tiempo_inicial
segundos: Long = 40

scala> val minutos = segundos / 60
minutos: Long = 0

scala> writer.write("\nEn el nuevo train las instancias son: " + reducedTrain10.count() +  "\n")

scala> writer.write("En el nuevo test las instancias son: " + reducedTest10.count())

scala> writer.write("\nFin de selección de características con los datos discretizados y c on una duración de: " + segundos + " segundos o de " +  minutos + " minutos" + "\n")

scala> 

scala> 

scala> //RANDOM FOREST 2 CON SELECCIÓN DE CARACTERÍSTICAS

scala> writer.write("\nComienzo Random Forest 2 (Selección de características) con los sig uientes parámetros:" + "\n")

scala> val tiempo_inicial = System.currentTimeMillis / 1000
tiempo_inicial: Long = 1494922546

scala> val numClasses = 2
numClasses: Int = 2

scala> val categoricalFeaturesInfo = Map[Int, Int]()
categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()

scala> val numTrees = 10 
numTrees: Int = 10

scala> val featureSubsetStrategy = "auto" 
featureSubsetStrategy: String = auto

scala> val impurity = "gini"
impurity: String = gini

scala> val maxDepth = 4
maxDepth: Int = 4

scala> val maxBins = 32
maxBins: Int = 32

scala> writer.write("\tNumClasses: " + numClasses + "\n\tcategoricalFeaturesInfo: Map[Int,  Int]()" + "\n\tnumTrees: " + numTrees + "\n\tfeatureSubsetStrategy: " + featureSubsetStra tegy + "\n\timpurity: " + impurity + "\n\tmaxDepth: " + maxDepth + "\n\tmaxBins: " + maxBi ns + "\n")

scala> val model = RandomForest.trainClassifier(reducedTrain10, numClasses, categoricalFea turesInfo,
     |   numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)
model: org.apache.spark.mllib.tree.model.RandomForestModel = 
TreeEnsembleModel classifier with 10 trees


scala> //Evaluamos el modelo con el test

scala> val predsAndLabels = reducedTest10.map { point =>
     |   val prediction = model.predict(point.features)
     |   (prediction, point.label)
     | }
predsAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[152] at map at <console>:105

scala> //Obtenemos el test error

scala> writer.write("Resultados:")

scala> val testErr_rf2 = predsAndLabels.filter(r => r._1 != r._2).count.toDouble / reduced Test10.count()
testErr_rf2: Double = 0.3002791806822478

scala> writer.write("\n\tTest Error= " + testErr_rf2)

scala> //println("Learned model:\n" + model.toDebugString) //Mostramos el modelo COMENTADO 

scala> //Mostramos los resultados pormenorizados

scala> val metrics_rf2 = new MulticlassMetrics(predsAndLabels)
metrics_rf2: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@5e111568

scala> writer.write("\n\tPrecisión: " + metrics_rf2.precision)

scala> writer.write("\n\tMatriz de confusión: \n" + metrics_rf2.confusionMatrix )

scala> val binaryMetrics_rf2 = new BinaryClassificationMetrics(predsAndLabels)
binaryMetrics_rf2: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@72b76a87

scala> val area_under_ROC_rf2 = binaryMetrics_rf2.areaUnderROC
area_under_ROC_rf2: Double = 0.5707932720672522

scala> writer.write("\n\tArea bajo la curva ROC = " + area_under_ROC_rf2)

scala> val segundos = (System.currentTimeMillis / 1000) - tiempo_inicial
segundos: Long = 7

scala> val minutos = segundos / 60
minutos: Long = 0

scala> writer.write("\nFin de Random Forest 2 (selección de características) con una durac ión de: " + segundos + " segundos o de " +  minutos + " minutos" + "\n\n")

scala> 

scala> 

scala> //NAIVE BAYES 2 CON SELECCIÓN DE CARACTERÍSTICAS

scala> writer.write("Comienzo Naive Bayes 2 (Selección de Características) con los siguien tes parámetros:" + "\n")

scala> val tiempo_inicial = System.currentTimeMillis / 1000
tiempo_inicial: Long = 1494922554

scala> //Creamos el modelo

scala> val miLambda = 1.0
miLambda: Double = 1.0

scala> val miModelType = "multinomial"
miModelType: String = multinomial

scala> writer.write("\tlambda: " + miLambda + "\n\tmodelType: " + miModelType + "\n")

scala> val model = NaiveBayes.train(reducedTrain10, lambda = miLambda, modelType = miModel Type)
model: org.apache.spark.mllib.classification.NaiveBayesModel = org.apache.spark.mllib.classification.NaiveBayesModel@7fac74ba

scala> //Evaluamos el modelo

scala> val predsAndLabels = reducedTest10.map { point =>
     |   val prediction = model.predict(point.features)
     |   (prediction, point.label)
     | }
predsAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[178] at map at <console>:95

scala> //Obtenemos el test error

scala> writer.write("Resultados:")

scala> val testErr_nb2 = predsAndLabels.filter(r => r._1 != r._2).count.toDouble / reduced Test10.count()
testErr_nb2: Double = 0.29629746313942557

scala> writer.write("\n\tTestError= " + testErr_nb2)

scala> //Mostramos los resultados pormenorizados

scala> val metrics_nb2 = new MulticlassMetrics(predsAndLabels)
metrics_nb2: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@6b047c90

scala> writer.write("\n\tPrecisión: " + metrics_nb2.precision)

scala> writer.write("\n\tMatriz de confusión: \n" + metrics_nb2.confusionMatrix )

scala> val binaryMetrics_nb2 = new BinaryClassificationMetrics(predsAndLabels)
binaryMetrics_nb2: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@6e58d931

scala> val area_under_ROC_nb2 = binaryMetrics_nb2.areaUnderROC
area_under_ROC_nb2: Double = 0.651367173702239

scala> writer.write("\n\tArea bajo la curva ROC = " + area_under_ROC_nb2)

scala> val segundos = (System.currentTimeMillis / 1000) - tiempo_inicial
segundos: Long = 2

scala> val minutos = segundos / 60
minutos: Long = 0

scala> writer.write("\nFin de Naive Bayes 2 con una duración de: " + segundos + " segundos  o de " +  minutos + " minutos" + "\n")

scala> 

scala> 

scala> //DECISION TREES 2 CON SELECCIÓN DE CARACTERÍSTICAS

scala> writer.write("\nComienzo Decision Trees 2 con los siguientes parámetros:" + "\n")

scala> val tiempo_inicial = System.currentTimeMillis / 1000
tiempo_inicial: Long = 1494922557

scala> //Configuramos el modelo y lo entrenamos

scala> val numClasses = 2
numClasses: Int = 2

scala> val categoricalFeaturesInfo = Map[Int, Int]()
categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()

scala> val impurity = "gini"
impurity: String = gini

scala> val maxDepth = 4
maxDepth: Int = 4

scala> val maxBins = 32
maxBins: Int = 32

scala> writer.write("\tNumClasses: " + numClasses + "\n\tcategoricalFeaturesInfo: Map[Int,  Int]()" + "\n\timpurity: " + impurity + "\n\tmaxDepth: " + maxDepth + "\n\tmaxBins: " + m axBins + "\n")

scala> val model = DecisionTree.trainClassifier(reducedTrain10, numClasses, categoricalFea turesInfo,
     |   impurity, maxDepth, maxBins)
model: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 4 with 31 nodes

scala> //Evaluamos el modelo con el test

scala> val predsAndLabels = reducedTest10.map { point =>
     |   val prediction = model.predict(point.features)
     |   (prediction, point.label)
     | }
predsAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[222] at map at <console>:101

scala> //Obtenemos el test error

scala> writer.write("Resultados:")

scala> val testErr_dt2 = predsAndLabels.filter(r => r._1 != r._2).count.toDouble / reduced Test10.count() 
testErr_dt2: Double = 0.2910434377999011

scala> writer.write("\n\tTestError= " + testErr_dt2)

scala> //println("Learned model:\n" + model.toDebugString) //Mostramos el modelo

scala> //Mostramos los resultados pormenorizados

scala> val metrics_dt2 = new MulticlassMetrics(predsAndLabels)
metrics_dt2: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@27ce710f

scala> writer.write("\n\tPrecisión: " + metrics_dt2.precision)

scala> writer.write("\n\tMatriz de confusión: \n" + metrics_dt2.confusionMatrix )

scala> val binaryMetrics_dt2 = new BinaryClassificationMetrics(predsAndLabels)
binaryMetrics_dt2: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@41394a58

scala> val area_under_ROC_dt2 = binaryMetrics_dt2.areaUnderROC
area_under_ROC_dt2: Double = 0.6176780480710617

scala> writer.write("\n\tArea bajo la curva ROC = " + area_under_ROC_dt2)

scala> val segundos = (System.currentTimeMillis / 1000) - tiempo_inicial
segundos: Long = 3

scala> val minutos = segundos / 60
minutos: Long = 0

scala> writer.write("\nFin de Decision Trees 2 con una duración de: " + segundos + " segun dos o de " +  minutos + " minutos" + "\n\n")

scala> 

scala> writer.close()

scala> 

scala> reducedTest10.saveAsTextFile("hdfs://hadoop-master/user/mdat20076629/DTSpark/test10 /");

scala> reducedTrain10.saveAsTextFile("hdfs://hadoop-master/user/mdat20076629/DTSpark/train 10/");

scala> reducedTrain10.repartition(6).coalesce(1, shuffle = true).saveAsTextFile("hdfs://ha doop-master/user/mdat20076629/DTSpark/train10.data")

scala> reducedTest10.repartition(6).coalesce(1, shuffle = true).saveAsTextFile("hdfs://had oop-master/user/mdat20076629/DTSpark/test10.data")

scala> 

scala> 

scala> 

scala> exit
warning: there were 1 deprecation warning(s); re-run with -deprecation for details
